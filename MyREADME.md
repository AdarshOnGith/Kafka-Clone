# DistributedMQ - Microservices Architecture

## Modern Cloud-Native Distributed Messaging System

A production-grade-like, microservices-based distributed messaging system inspired by Apache Kafka, built with modern architectural patterns and cloud-native principles.

---

## ðŸ—ï¸ Architecture Overview

### Microservices-Based Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Client                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Producer Client  â”‚              â”‚ Consumer Client  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚        â”‚
                â”Œâ”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                                            â”‚    
                |                                            |
                |                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                |                               â”‚  Storage Service        â”‚
                |                               â”‚  (Multiple Nodes)       |
                â”‚                               |[API-GateWay like logic] |
                |                               â”‚  - Leader/Follower      â”‚
                |                               â”‚  - WAL Storage          â”‚
                |                               â”‚  - Replication          â”‚
                |                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                |
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
|        Metadata Service         |
|        (Multiple Nodes)         |
|      [API-GateWay like logic]   | 
|    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  |  
|    â”‚  Metadata part          â”‚  |
|    â”‚  - Topic Metadata       â”‚  |
|    â”‚  - Partition Leaders    â”‚  |
|    â”‚  - Consumer Offsets     â”‚  |
|    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  |
|    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  |
|    â”‚  Controller part        â”‚  |
|    â”‚  - Failure Detection    â”‚  |
|    â”‚  - Leader Election      â”‚  |
|    â”‚  - Cluster Coordination â”‚  |
|    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  |
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                
```

---

## ðŸŽ¯ Key Features

### 1. **Microservices Architecture**
### 2. **High Throughput & Low Latency**
### 3. **Fault Tolerance**s
### 4. **Scalability**

---
### Admin client for cluster management
### 1. Producer Client side responsibilities
**Responsibilities:**
- Receive messages from producers(initiator-service/client)
- Reads/req metadata from metadata service
- Apply partitioning logic (hash-based or custom)
- Batch messages for efficiency
- Route messages to correct storage nodes
- gets acknowledgments

**Key Operations:**
- Partition assignment
- Leader discovery via Metadata Service
- Message validation
- Batching and compression

---

### 2. Consumer Client side responsibilities (to cross check)
**Responsibilities:**
- Handle consumer subscriptions
- Manage consumer groups
- Track partition assignments
- Coordinate consumer rebalancing
- Deliver messages to consumers

**Key Operations:**
- Consumer group coordination
- Partition assignment
- Offset management
- Rebalancing protocol
---

## ðŸ“¦ Microservice Breakdown

## ðŸ§  Functional Breakdown

### 1. ðŸ“‡ Metadata Service

#### a. **Metadata Subsystem**
**Responsibilities:**
- Track topic, partition, and leader information
- Store and serve metadata to producers/consumers
- Maintain consumer group offsets
- Provide discovery for storage nodes

**Data Stored:**
- Topics and partitions
- Partition leaders and ISR list
- Consumer group offsets
- Cluster topology

**Storage:** PostgreSQL or similar relational DB

---

#### b1. **Controller Subsystem**
**Responsibilities:**
- Detect broker/storage node failures
- Perform partition leader elections
- Coordinate replication and ISR tracking
- Update metadata based on cluster state changes

**Components:**
- Write-Ahead Log (WAL) for changes
- Leader/follower coordination
- Heartbeat monitoring
- Leader election logic
- Metadata broadcasting to all nodes
---

#### b2. **Cluster Coordination Subsystem**
**Responsibilities:**
- Distributed locking
- Leader election for controller role
- Ephemeral node tracking
- Configuration synchronization across nodes
---

### 2. ðŸ—„ï¸ Storage Service

**Responsibilities:**
- Persist messages using Write-Ahead Log (WAL)
- Handle partition leadership (leader/follower role duties)
- Replicate data to ISR nodes
- Serve read requests to consumers
- Manage log retention and compaction
---
---
Note: Each microservice has a logic at entry point, where it performs sanity checks and req validations before processing requests.
- Use Java/spring and maven
- Use a layered architechture for each module.

---



## ðŸ”„ Core System Flows

---

### ðŸ” Flow 1: Producer Publishes Message (Write Path)

```
Producer Client
    â”‚
    â”‚ 
    â–¼
Producer Ingestion Service
    â”‚
    â”‚ 1. Partition assignment (hash-based)
    â”‚ 2. Group by partition
    â”‚ 3. Query Metadata Service for leaders
    |
    |
    | n/w call
    â–¼
API-gateway-like layer of metadata service
Metadata Service
    â”‚
    â”‚ returns metadata requested.
    â–¼
Producer Ingestion Service
    â”‚
    â”‚ n/w call to storage node(partition leader)
    â–¼
Storage Service (Leader)
    â”‚
    â”‚ 1. Append to local WAL
    â”‚ 2. Replicate to followers
    â”‚ 3. Wait for ISR acks
    â”‚ 4. Return success
    â–¼
Response chain back to Producer Client
```

#### ðŸ” Additional Notes:
- Producer initially uses **bootstrap metadata nodes** to fetch metadata.
- On metadata fetch:
  - Metadata service validates the request.
  - If the topic doesn't exist, it routes to controller to create it.
- Uses metadata to get partition leader and target broker (storage node).
- Storage node validates and processes the produce request.

---

### ðŸ§± Kafka-Inspired Internal Broker Logic (Simplified)

#### Kafka-Inspired Steps:

1. **Receive & Parse Request**
   - Authn/Authz
   - Parse topic, partition, records, acks, producer ID/epoch, txn info

2. **Validation & Quotas**
   - Check topic/partition existence
   - Authorization & quotas
   - Idempotency checks

3. **Append to WAL**
   - Assign offsets
   - Write to local log segment

4. **Replication to ISR**
   - Followers fetch data from leader
   - Leader tracks high watermark (HW)

5. **Acknowledge Based on `acks`:**

| Acks Setting | Behavior                             |
|--------------|--------------------------------------|
| `acks=0`     | Return immediately                   |
| `acks=1`     | Return after write to leader         |
| `acks=all`   | Return after all ISRs replicate      |

6. **Update HW & LEO**
   - HW = last offset replicated to all ISRs
   - LEO = next offset to be written

7. **Send Response to Producer**
   - Includes topic, partition, base offset, errors if any

8. **Consumer Visibility**
   - Only messages up to HW are fetchable

---

## âš™ï¸ When Is Metadata Updated?

Metadata is updated:
- When topics/partitions are created or deleted
- During leader election
- When ISR list changes
    Leader sends updated ISR list to the controller.
    Controller updates cluster metadata (ISR, leader info, etc.).
    Updated metadata is propagated to all metadata brokers.
    Metadata brokers update caches and respond with the latest cluster state to producers and consumers.
- On configuration changes

> âœ… **HW/LEO are local states**, not propagated as cluster metadata  
> ðŸš« **Metadata is not updated during normal produce flow**

---

---

### Flow 2: Consumer Reads Message (Read Path)

```
Consumer Client
    â”‚
    â”‚ 
    â–¼
Consumer Egress Service
    â”‚
    â”‚ 1. Check consumer group membership
    â”‚ 2. Get partition assignment
    â”‚ 3. Query Metadata Service for offset & leader
    |
    |  n/w call to metadata service
    |
    â–¼
Metadata Service
    â”‚
    â”‚ Return requested metadata
    â–¼
Consumer Egress Service
    â”‚
    â”‚ n/w call to storage (leader)
    â–¼
Storage Service (Leader)
    â”‚
    â”‚ Read from WAL at offset
    â–¼
Response chain back to Consumer Client
    â”‚
Consumer processes messages
    â”‚
    â–¼
Consumer Egress Service
    â”‚
    â”‚ Update offset in Metadata Service
    â–¼
Offset committed
```

---

## ðŸ”„ Flow 3: Cluster Self-Healing (Failure Recovery)

```
Controller Service
    â”‚
    â”‚ Monitor heartbeats / Watch nodes
    â–¼
Detect Storage Node Failure
    â”‚
    â”‚ Query Metadata part for affected partitions
    â–¼
For each partition:
    â”‚
    â”‚ 1. Get ISR list
    â”‚ 2. Select new leader from ISR
    â”‚ 3. Update Metadata Service
    â–¼
Metadata Service(s) updated and sync-ed
    â”‚
    â”‚ New leader address stored
    â–¼
Client services refresh metadata cache
    â”‚
    â”‚ Next requests route to new leader
    â–¼
Cluster healed

```

---

### ðŸ§  Controller Election & Recovery Details

- All controller nodes participate in a **Raft quorum**.
- The **current controller is the Raft leader**.
- If controller fails:
  - Raft detects failure via missed heartbeats
  - Remaining nodes perform **automatic leader election**
  - New Raft leader becomes the **active controller**

**Responsibilities of New Controller:**
- Resume partition leader election
- ISR management
- Metadata propagation
- Cluster-wide coordination
- basically take place of old controller

---

## ðŸ“Š Project Structure

```
DistributedMQ/
â”œâ”€â”€ dmq-common/                    # Shared libraries
â”‚           # Data models, # Protocol Buffers definitions, # Utilities etc.
â”‚
â”‚----dmq-client/   
|    â”œâ”€â”€ dmq-producer-client/        # Producer Ingestion Service
|    â”‚   â””â”€â”€ src/main/java/
|    â”‚       â””â”€â”€ com/distributedmq/producer/
|    â”‚
|    â”œâ”€â”€ dmq-consumer-client/          # Consumer Egress Service
|        â””â”€â”€ src/main/java/
|            â””â”€â”€ com/distributedmq/consumer/
â”‚
â”œâ”€â”€ dmq-metadata-service/         # Metadata Service
|    â”œâ”€â”€ dmq-metadata-handler/         # Metadata part
|    â””â”€â”€ dmq-controller-handler/       # Controller and coordination part
|
â”œâ”€â”€ dmq-storage-service/          # Storage Service
â”‚   â””â”€â”€ src/main/java/
â”‚       â””â”€â”€ com/distributedmq/storage/
|
â”œâ”€â”€ Admin_Client/  # Very last part to do
â”œâ”€â”€ docker/                       # Docker configurations
â”œâ”€â”€ kubernetes/                   # K8s manifests
â””â”€â”€ docs/                         # Documentation
```

---

## ðŸš€ Getting Started

### Prerequisites
- Java 17+
- Maven 3.8+
- Docker & Docker Compose
- PostgreSQL 14+

### Quick Start (Local Development)

```bash
# Start infrastructure
docker-compose up -d postgres etcd

# Build all services
mvn clean install

```