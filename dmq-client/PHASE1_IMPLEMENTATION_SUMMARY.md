# Consumer Client Library - Phase 1 Implementation Summary

## âœ… Implementation Complete - October 17, 2025

### Architecture: Client-Side Consumer Library with Server-Side Metadata Service

---

## ğŸ¯ Phase 1 Scope

**Goal:** Single consumer per group, simple message consumption

**What's Implemented:**
- âœ… Join consumer group via Consumer Egress Service (CES)
- âœ… Get partition metadata (leader, currentOffset, highWaterMark, ISR)
- âœ… Poll messages from storage nodes
- âœ… Track fetch positions locally
- âœ… Seek operations

**What's Deferred to Phase 2:**
- â³ Multi-member consumer groups
- â³ Client-side rebalancing
- â³ Offset commit
- â³ Heartbeat mechanism
- â³ Leave group

---

## ğŸ“¡ API Contract with Consumer Egress Service

### **Endpoint:** `POST /api/consumer/join-group`

**Request:**
```json
{
  "groupId": "my-consumer-group",
  "consumerId": "consumer-abc123",
  "topics": ["orders", "payments"]
}
```

**Response from CES:**
```json
{
  "success": true,
  "groupId": "my-consumer-group",
  "partitions": [
    {
      "topic": "orders",
      "partition": 0,
      "leader": {
        "brokerId": 1,
        "host": "storage-node-1",
        "port": 9092
      },
      "currentOffset": 100,
      "highWaterMark": 250,
      "isr": [1, 2, 3]
    },
    {
      "topic": "orders",
      "partition": 1,
      "leader": {
        "brokerId": 2,
        "host": "storage-node-2",
        "port": 9092
      },
      "currentOffset": 50,
      "highWaterMark": 150,
      "isr": [2, 3]
    },
    {
      "topic": "payments",
      "partition": 0,
      "leader": {...},
      "currentOffset": 0,
      "highWaterMark": 80,
      "isr": [1, 3]
    }
  ]
}
```

### **CES Responsibilities:**

1. **Group Metadata (No Member Tracking):**
   - CES does **NOT** track which consumers are in the group
   - CES only knows: "group X is consuming topic Y"
   - Member assignment happens entirely in client library

2. **Partition Metadata Query:**
   - Query Metadata Service for topic partition info
   - Get leader broker for each partition
   - Get current offset for this group:
     - If offset committed â†’ return committed offset
     - If no committed offset â†’ return earliest available offset (based on policy)
   - Get high watermark (latest offset)
   - Get ISR list

3. **Response:**
   - Return complete partition metadata for requested topics
   - Client library decides which partitions to consume

**Note:** Client library handles all member assignment logic locally!

---

## ğŸ”„ Complete Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLIENT APPLICATION                                         â”‚
â”‚                                                             â”‚
â”‚  ConsumerConfig config = ConsumerConfig.builder()          â”‚
â”‚      .metadataServiceUrl("http://localhost:8081")          â”‚
â”‚      .groupId("my-group")                                  â”‚
â”‚      .build();                                             â”‚
â”‚                                                             â”‚
â”‚  Consumer consumer = new DMQConsumer(config);              â”‚
â”‚  consumer.subscribe(Arrays.asList("orders"));              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DMQConsumer (Client Library)                               â”‚
â”‚                                                             â”‚
â”‚  1. Generate consumerId = "consumer-abc123"                â”‚
â”‚  2. Build request: {groupId, consumerId, topics}           â”‚
â”‚  3. Call ConsumerEgressClient.joinGroup()                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚ HTTP POST /api/consumer/join-group
                           â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  CONSUMER EGRESS SERVICE (Server-Side)                      â•‘
â•‘                                                             â•‘
â•‘  1. Receive join request                                   â•‘
â•‘  2. Check if group "my-group" has committed offsets        â•‘
â•‘     (CES does NOT track members!)                          â•‘
â•‘  3. Query Metadata Service:                                â•‘
â•‘     - How many partitions for "orders"?                    â•‘
â•‘     - Who is leader for each partition?                    â•‘
â•‘     - What's committed offset for "my-group"?              â•‘
â•‘     - What's high watermark for each partition?            â•‘
â•‘     - What's ISR for each partition?                       â•‘
â•‘  4. Build and return response (partition metadata only)    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                           â”‚
                           â”‚ HTTP Response
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DMQConsumer (Client Library)                               â”‚
â”‚                                                             â”‚
â”‚  6. Receive partition metadata                             â”‚
â”‚  7. CLIENT-SIDE: Decide which partitions to consume        â”‚
â”‚     Phase 1: Consume ALL partitions (single member)        â”‚
â”‚     Phase 2: Use rebalancing algorithm                     â”‚
â”‚  8. Store metadata for assigned partitions:                â”‚
â”‚     - Leader broker address                                â”‚
â”‚     - Current offset (from CES)                            â”‚
â”‚     - High watermark                                       â”‚
â”‚     - ISR                                                  â”‚
â”‚  9. Initialize fetch positions = currentOffset             â”‚
â”‚  10. Ready to poll!                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚ consumer.poll(1000)
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DMQConsumer.poll()                                         â”‚
â”‚                                                             â”‚
â”‚  For EACH partition:                                       â”‚
â”‚    - Get leader address from metadata                      â”‚
â”‚    - Get current fetch offset                              â”‚
â”‚    - HTTP POST to Storage Service:                         â”‚
â”‚      http://storage-node-1:9092/api/storage/fetch          â”‚
â”‚    - Receive messages                                      â”‚
â”‚    - Update fetch position                                 â”‚
â”‚                                                             â”‚
â”‚  Return all messages to client                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Files Updated

### **DTOs** (`dmq-common/dto/`)

1. **`ConsumerSubscriptionRequest.java`**
   ```java
   {
     groupId: "my-group",
     consumerId: "consumer-abc123",
     topics: ["orders"],
     // Future fields kept for Phase 2
     clientId, sessionTimeoutMs, heartbeatIntervalMs, autoOffsetReset
   }
   ```

2. **`ConsumerSubscriptionResponse.java`**
   ```java
   {
     success: true,
     groupId: "my-group",
     partitions: [PartitionMetadata...],
     // Future fields kept for Phase 2
     consumerId, generationId, coordinatorHost, coordinatorPort
   }
   ```

### **Models** (`dmq-common/model/`)

3. **`PartitionMetadata.java`** (Updated)
   ```java
   {
     topic: "orders",
     partition: 0,
     leader: BrokerNode,
     currentOffset: 100,      // CES provides this
     highWaterMark: 250,      // NEW: for lag monitoring
     isr: [1, 2, 3],          // NEW: ISR list
     // Future fields kept
     replicas, startOffset, endOffset
   }
   ```

### **Client** (`dmq-client/consumer/`)

4. **`ConsumerEgressClient.java`** (Updated)
   - âœ… `joinGroup()` - Join consumer group (Phase 1)
   - âœ… `fetchMessages()` - Fetch from storage nodes
   - â³ `commitOffsets()` - Kept for Phase 2
   - â³ `sendHeartbeat()` - Kept for Phase 2
   - â³ `leaveGroup()` - Kept for Phase 2

5. **`DMQConsumer.java`** (Updated)
   - âœ… `subscribe()` - Join group and get partition metadata
   - âœ… `poll()` - Fetch messages from all partitions
   - âœ… `seek()` - Seek to specific offset
   - âœ… `seekToBeginning()` / `seekToEnd()` - Seek operations
   - â³ `commitSync()` / `commitAsync()` - Stubbed for Phase 2
   - â³ `close()` - Simple cleanup (no leave group in Phase 1)

---

## ğŸ® Usage Example

```java
// 1. Configure consumer
ConsumerConfig config = ConsumerConfig.builder()
    .metadataServiceUrl("http://localhost:8081")  // CES URL
    .groupId("order-processor-group")
    .clientId("order-processor")
    .build();

// 2. Create consumer
Consumer consumer = new DMQConsumer(config);

// 3. Subscribe (joins group, gets partition metadata)
consumer.subscribe(Arrays.asList("orders", "payments"));

// 4. Poll messages
while (true) {
    List<Message> messages = consumer.poll(1000);
    
    for (Message msg : messages) {
        // Process message
        System.out.println("Topic: " + msg.getTopic() + 
                          ", Partition: " + msg.getPartition() + 
                          ", Offset: " + msg.getOffset() +
                          ", Value: " + new String(msg.getValue()));
    }
    
    // Phase 2: Commit offsets
    // consumer.commitSync();
}

// 5. Close
consumer.close();
```

---

## ğŸ”‘ Key Design Decisions

### **1. CES Provides Partition Metadata Only**
- âœ… CES is stateless - no member tracking
- âœ… CES only provides: partition leaders, offsets, ISR
- âœ… Client library decides partition assignment

### **2. Client-Side Member Assignment**
- âœ… Phase 1: Single consumer = consume all partitions
- âœ… Phase 2: Multiple consumers = client-side rebalancing algorithm
- âœ… No server coordination needed

### **3. highWaterMark Added**
- âœ… For consumer lag monitoring
- âœ… For seekToEnd() operation
- âœ… Shows how far behind consumer is

### **4. Future-Proof DTOs**
- âœ… All Phase 2 fields kept but unused
- âœ… Easy to extend without breaking changes
- âœ… Comments indicate Phase 1 vs Phase 2

---

## âš¡ Phase 1 Limitations

| Feature | Status | Notes |
|---------|--------|-------|
| Single consumer per group | âœ… Works | Only one member allowed |
| Multi-member groups | âŒ Phase 2 | Rebalancing needed |
| Offset commit | âŒ Phase 2 | CES endpoint not called |
| Heartbeat | âŒ Phase 2 | No health monitoring |
| Leave group | âŒ Phase 2 | Consumer just closes |
| Auto-commit | âŒ Phase 2 | Manual only |

---

## ğŸš€ CES Implementation Guide (For Reference Only - Not Your Task)

### **Consumer Egress Service - What It Does:**

CES is a **stateless metadata gateway**. It does NOT track members!

```java
@RestController
@RequestMapping("/api/consumer")
public class ConsumerController {
    
    @PostMapping("/join-group")
    public ResponseEntity<ConsumerSubscriptionResponse> joinGroup(
        @RequestBody ConsumerSubscriptionRequest request) {
        
        String groupId = request.getGroupId();
        List<String> topics = request.getTopics();
        
        // CES does NOT track members - just returns metadata!
        
        // 1. Query Metadata Service for each topic:
        //    - Partition count
        //    - Leader broker for each partition
        //    - Committed offset for this group (or earliest if none)
        //    - High watermark
        //    - ISR list
        
        // 2. Build response with partition metadata
        List<PartitionMetadata> partitions = new ArrayList<>();
        
        for (String topic : topics) {
            // Query metadata service...
            for (int partitionId : partitions) {
                partitions.add(PartitionMetadata.builder()
                    .topic(topic)
                    .partition(partitionId)
                    .leader(leaderBroker)
                    .currentOffset(committedOffset != null ? committedOffset : earliestOffset)
                    .highWaterMark(latestOffset)
                    .isr(isrList)
                    .build());
            }
        }
        
        // 3. Return metadata - client decides what to do with it!
        return ResponseEntity.ok(ConsumerSubscriptionResponse.builder()
            .success(true)
            .groupId(groupId)
            .partitions(partitions)
            .build());
    }
}
```

**Note:** This is NOT your responsibility - someone else implements CES!

---

## ğŸ“Š Testing Checklist

- [ ] Consumer subscribes to single topic
- [ ] Consumer subscribes to multiple topics
- [ ] Consumer polls messages successfully
- [ ] Fetch position advances after poll
- [ ] Seek operations work correctly
- [ ] Consumer handles empty poll gracefully
- [ ] Consumer handles network errors
- [ ] Multiple consumers in different groups work independently
- [ ] CES creates group if not exists
- [ ] CES returns committed offset if available
- [ ] CES returns earliest offset if no commit

---

## ğŸ“ Phase 2 Planning

### **Multi-Member Consumer Groups (Client-Side Logic):**

1. **Local Member Registry:**
   - Client library tracks local consumers (in-memory)
   - Each consumer instance knows about others in same JVM
   - No server-side coordination

2. **Client-Side Rebalancing Algorithm:**
   - When new consumer starts: redistribut partitions locally
   - Use round-robin, range, or consistent hashing
   - Each consumer independently decides its partitions

3. **Example:**
   ```
   Consumer-1 and Consumer-2 in same JVM, same group
   Topic "orders" has 3 partitions
   
   Client library logic:
   - Consumer-1 gets partitions [0, 1]
   - Consumer-2 gets partition [2]
   
   No server involved in this decision!
   ```

3. **Offset Commit:**
   - Implement commit endpoint
   - Store offsets in metadata service
   - Handle commit failures

4. **Heartbeat:**
   - Background thread sends heartbeats
   - CES marks members as dead if no heartbeat
   - Triggers rebalance

---

**Phase 1 Complete! âœ…**  
**Ready for integration testing with CES!** ğŸš€
